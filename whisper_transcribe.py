#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üéôÔ∏è –ú–∞—Å—Å–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é OpenAI Whisper üéôÔ∏è

–≠—Ç–æ—Ç Python-—Å–∫—Ä–∏–ø—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ (mp3, wav, m4a)
–≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å OpenAI Whisper –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Ä–µ—á–∏.
–°–∫—Ä–∏–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ GPU NVIDIA –¥–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è.

–ù–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ: –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA, CUDA –∏ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA.

–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω.
- –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –≤ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞:
  - .txt: —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞.
  - .srt: —Ñ–∞–π–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.
  - all_transcripts.txt: –æ–±—â–∏–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
- –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã.

–ü–æ—Ä—è–¥–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
1. –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: source .venv/bin/activate
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç, —É–∫–∞–∑–∞–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ:
   python whisper_transcribe.py <–ø—É—Ç—å_–∫_–∞—É–¥–∏–æ> <–º–æ–¥–µ–ª—å> <–ø–∞–ø–∫–∞_—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤>
3. –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

–ê–≤—Ç–æ—Ä: –ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω https://shardin.name/
–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 29.08.2025
–í–µ—Ä—Å–∏—è: 2.0

–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –≤—Å–µ–≥–¥–∞ –∑–¥–µ—Å—å: https://github.com/empenoso/offline-audio-transcriber

"""

import os
import sys
import glob
import json
import time
from pathlib import Path
import whisper
import torch
import io

# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–∏ –∏–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
import librosa
import soundfile as sf
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def check_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ GPU —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")
        return False
    
    try:
        gpu_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üéÆ –ù–∞–π–¥–µ–Ω GPU: {gpu_name}")
        print(f"üíæ –ü–∞–º—è—Ç—å GPU: {memory_gb:.1f} GB")
        
        # –¢–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ GPU - —Å–æ–∑–¥–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ç–µ–Ω–∑–æ—Ä
        test_tensor = torch.zeros(10, 10).cuda()
        _ = test_tensor + 1  # –ü—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
        test_tensor = test_tensor.cpu()  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        del test_tensor
        torch.cuda.empty_cache()
        
        print("‚úÖ GPU —Å–æ–≤–º–µ—Å—Ç–∏–º —Å PyTorch")
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è  GPU –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ç–µ–∫—É—â–∏–º PyTorch: {str(e)}")
        print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU —Ä–µ–∂–∏–º")
        return False

def load_whisper_model(model_size="medium", use_gpu=True):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ GPU"""
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper ({model_size})...")
    
    device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
    
    try:
        model = whisper.load_model(model_size, device=device)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
        return model, device
    except Exception as e:
        if device == "cuda":
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ GPU: {str(e)}")
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU...")
            model = whisper.load_model(model_size, device="cpu")
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            return model, "cpu"
        else:
            raise e

def get_audio_files(directory):
    """–ü–æ–∏—Å–∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    audio_extensions = ['.wav', '.mp3', '.m4a']
    files = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        if os.path.isfile(file_path):
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext in audio_extensions:
                files.append(file_path)
    
    return sorted(files)

def transcribe_audio(model, file_path, device="cpu", language="ru"):
    """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
    print(f"üéµ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {os.path.basename(file_path)}")
    
    try:
        start_time = time.time()
        
        result = model.transcribe(
            file_path, 
            language=language,
            verbose=False,
            fp16=device == "cuda"
        )
        
        processing_time = time.time() - start_time
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Å–µ–≥–º–µ–Ω—Ç—ã
        text = result["text"].strip()
        segments = result.get("segments", [])
        
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ –∑–∞ {processing_time:.1f}—Å")
        
        return {
            "file": file_path,
            "text": text,
            "segments": segments,
            "language": result.get("language", language),
            "processing_time": processing_time
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
        return None

def get_audio_duration(file_path):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
    try:
        y, sr = librosa.load(file_path, sr=None)
        duration = len(y) / sr
        return duration
    except:
        return 0

def fast_kmeans_diarization(audio_path, n_speakers=2, min_segment_duration=1.0):
    """
    –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ K-Means –∏ MFCC
    """
    try:
        print("üîä –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è (K-Means)...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        y, sr = librosa.load(audio_path, sr=16000)
        duration = len(y) / sr
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ MFCC features
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)
        mfccs = mfccs.T  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        X = scaler.fit_transform(mfccs)
        
        # K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º Spectral)
        kmeans = KMeans(n_clusters=n_speakers, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        hop_duration = 512 / sr  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        segments = []
        current_speaker = None
        current_start = 0
        
        for i, label in enumerate(labels):
            current_time = i * hop_duration
            
            if current_speaker is None:
                current_speaker = f"spk_{label + 1}"
                current_start = current_time
            elif f"spk_{label + 1}" != current_speaker:
                segment_duration = current_time - current_start
                if segment_duration >= min_segment_duration:
                    segments.append({
                        'speaker': current_speaker,
                        'start': current_start,
                        'end': current_time,
                        'duration': segment_duration
                    })
                current_speaker = f"spk_{label + 1}"
                current_start = current_time
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        if current_speaker is not None:
            segment_duration = duration - current_start
            if segment_duration >= min_segment_duration:
                segments.append({
                    'speaker': current_speaker,
                    'start': current_start,
                    'end': duration,
                    'duration': segment_duration
                })
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∑–∞ {duration:.1f}—Å –∞—É–¥–∏–æ")
        return segments
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return []

def energy_pitch_diarization(audio_path, n_speakers=2, min_segment_duration=1.0):
    """
    –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–µ—Ä–≥–∏–∏ –∏ –≤—ã—Å–æ—Ç—ã —Ç–æ–Ω–∞ (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è)
    """
    try:
        print("üîä –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Ç–æ–Ω—É...")
        
        y, sr = librosa.load(audio_path, sr=16000)
        
        # –î–µ—Ç–µ–∫—Ü–∏—è —Ä–µ—á–∏ –ø–æ —ç–Ω–µ—Ä–≥–∏–∏
        energy = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
        energy_threshold = np.percentile(energy, 25)
        speech_frames = energy > energy_threshold
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—ã—Å–æ—Ç—ã —Ç–æ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ—á–µ–≤—ã—Ö —Ñ—Ä–µ–π–º–æ–≤
        pitches = []
        times = []
        
        for i, is_speech in enumerate(speech_frames):
            if is_speech:
                start = i * 512
                end = start + 2048
                if end < len(y):
                    frame = y[start:end]
                    # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—Å–æ—Ç—ã —Ç–æ–Ω–∞
                    f0 = librosa.yin(frame, fmin=80, fmax=400, sr=sr)
                    if not np.isnan(f0[0]):
                        pitches.append(f0[0])
                        times.append(i * 512 / sr)
        
        if len(pitches) < 10:
            return []
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –≤—ã—Å–æ—Ç–µ —Ç–æ–Ω–∞
        pitches = np.array(pitches).reshape(-1, 1)
        kmeans = KMeans(n_clusters=n_speakers, random_state=42, n_init=5)
        pitch_labels = kmeans.fit_predict(pitches)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segments = []
        current_speaker = None
        current_start = 0
        
        for i, (time, label) in enumerate(zip(times, pitch_labels)):
            speaker_id = f"spk_{label + 1}"
            
            if current_speaker is None:
                current_speaker = speaker_id
                current_start = time
            elif speaker_id != current_speaker:
                if time - current_start >= min_segment_duration:
                    segments.append({
                        'speaker': current_speaker,
                        'start': current_start,
                        'end': time,
                        'duration': time - current_start
                    })
                current_speaker = speaker_id
                current_start = time
        
        print(f"‚úÖ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–æ–Ω—É: {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        return segments
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —Ç–æ–Ω—É: {e}")
        return []

def smart_diarization(audio_path, n_speakers=2, method="auto"):
    """
    –£–º–Ω—ã–π –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Ç—É–∞—Ü–∏–∏
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ
    y, sr = librosa.load(audio_path, sr=None)
    duration = len(y) / sr
    
    if method == "auto":
        if duration > 300:  # > 5 –º–∏–Ω—É—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥
            return energy_pitch_diarization(audio_path, n_speakers)
        elif duration > 60:  # 1-5 –º–∏–Ω—É—Ç - –±—ã—Å—Ç—Ä—ã–π K-Means
            return fast_kmeans_diarization(audio_path, n_speakers)
        else:  # < 1 –º–∏–Ω—É—Ç—ã - —Ç–æ—á–Ω—ã–π –º–µ—Ç–æ–¥
            return fast_kmeans_diarization(audio_path, n_speakers)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º K-Means –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    elif method == "fast":
        return energy_pitch_diarization(audio_path, n_speakers)
    elif method == "balanced":
        return fast_kmeans_diarization(audio_path, n_speakers)
    else:
        return fast_kmeans_diarization(audio_path, n_speakers)

def transcribe_with_diarization(model, audio_path, output_dir, language="ru", n_speakers=2):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
    """
    try:
        print(f"üé§ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è {n_speakers} –≥–æ–≤–æ—Ä—è—â–∏—Ö...")
        
        # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        segments = smart_diarization(audio_path, n_speakers, "balanced")
        
        if not segments:
            print("‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
        y, sr = librosa.load(audio_path, sr=16000)
        
        results = []
        total_segments = len(segments)
        
        for i, segment in enumerate(segments, 1):
            print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ {i}/{total_segments}...", end="", flush=True)
            
            start_sample = int(segment['start'] * sr)
            end_sample = int(segment['end'] * sr)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
            if start_sample >= len(y) or end_sample > len(y):
                continue
                
            segment_audio = y[start_sample:end_sample]
            
            if len(segment_audio) == 0:
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
            temp_path = os.path.join(output_dir, f"temp_segment_{i}.wav")
            sf.write(temp_path, segment_audio, sr)
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç
            try:
                result = model.transcribe(
                    temp_path,
                    language=language,
                    verbose=False,
                    fp16=torch.cuda.is_available()
                )
                
                results.append({
                    'speaker': segment['speaker'],
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': result['text'].strip(),
                    'segments': result.get('segments', [])
                })
                
                print("‚úÖ")
                
            except Exception as e:
                print(f"‚ùå: {e}")
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_path):
                    os.remove(temp_path)
        
        return results
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ transcribe_with_diarization: {e}")
        return None

def save_diarized_results(results, output_dir, base_name):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
    """
    if not results:
        return None, None, None
    
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö
    txt_path = os.path.join(output_dir, f"{base_name}_diarized.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {base_name}\n")
        f.write("=" * 50 + "\n\n")
        
        for result in results:
            start_min = int(result['start'] // 60)
            start_sec = int(result['start'] % 60)
            end_min = int(result['end'] // 60)
            end_sec = int(result['end'] % 60)
            
            f.write(f"[{result['speaker']}] {start_min:02d}:{start_sec:02d} - {end_min:02d}:{end_sec:02d}\n")
            f.write(f"{result['text']}\n\n")
    
    # 2. JSON —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    json_path = os.path.join(output_dir, f"{base_name}_diarized.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # 3. SRT —Å –º–µ—Ç–∫–∞–º–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö
    srt_path = os.path.join(output_dir, f"{base_name}_diarized.srt")
    with open(srt_path, 'w', encoding='utf-8') as f:
        for i, result in enumerate(results, 1):
            start = format_timestamp(result['start'])
            end = format_timestamp(result['end'])
            speaker = result['speaker']
            text = result['text']
            
            f.write(f"{i}\n{start} --> {end}\n[{speaker}] {text}\n\n")
    
    print(f"üíæ –§–∞–π–ª—ã –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {base_name}_diarized.*")
    return txt_path, json_path, srt_path

def save_single_result(result, output_dir, enable_diarization=False, model=None, n_speakers=2):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    if not result:
        return
        
    os.makedirs(output_dir, exist_ok=True)
    
    base_name = os.path.splitext(os.path.basename(result['file']))[0]
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    individual_txt = os.path.join(output_dir, f"{base_name}.txt")
    with open(individual_txt, 'w', encoding='utf-8') as f:
        f.write(result['text'])
    
    # SRT —Å—É–±—Ç–∏—Ç—Ä—ã (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã)
    if result['segments']:
        srt_path = os.path.join(output_dir, f"{base_name}.srt")
        with open(srt_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['segments'], 1):
                start = format_timestamp(segment['start'])
                end = format_timestamp(segment['end'])
                text = segment['text'].strip()
                f.write(f"{i}\n{start} --> {end}\n{text}\n\n")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Ñ–∞–π–ª
    all_txt_path = os.path.join(output_dir, "all_transcripts.txt")
    with open(all_txt_path, 'a', encoding='utf-8') as f:
        f.write(f"=== {os.path.basename(result['file'])} ===\n")
        f.write(f"{result['text']}\n\n")
    
    # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å)
    if enable_diarization and model:
        try:
            diarized_results = transcribe_with_diarization(
                model, result['file'], output_dir, language="ru", n_speakers=n_speakers
            )
            if diarized_results:
                save_diarized_results(diarized_results, output_dir, base_name)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {base_name}: {e}")
    
    print(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {base_name}.txt, {base_name}.srt")

def format_timestamp(seconds):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è SRT"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def save_final_json(results, output_dir):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON —Ñ–∞–π–ª–∞ —Å–æ –≤—Å–µ–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    json_path = os.path.join(output_dir, "transcripts_detailed.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

def print_statistics(results):
    """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    successful = [r for r in results if r is not None]
    failed = len(results) - len(successful)
    
    if successful:
        total_time = sum(r['processing_time'] for r in successful)
        avg_time = total_time / len(successful)
        total_text = sum(len(r['text']) for r in successful)
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(successful)} —Ñ–∞–π–ª–æ–≤")
        print(f"‚ùå –û—à–∏–±–æ–∫: {failed}")
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")
        print(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {avg_time:.1f}—Å")
        print(f"üìù –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {total_text}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üéôÔ∏è  –°–∫—Ä–∏–ø—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ —Å OpenAI Whisper\n")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    input_directory = "."  # –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
    output_directory = "transcripts"
    model_size = "large"  # tiny, base, small, medium, large
    language = "ru"  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
    enable_diarization = False  # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω–∞
    n_speakers = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        input_directory = sys.argv[1]
    if len(sys.argv) > 2:
        model_size = sys.argv[2]
    if len(sys.argv) > 3:
        output_directory = sys.argv[3]
    if len(sys.argv) > 4:
        enable_diarization = sys.argv[4].lower() in ['true', '1', 'yes', 'y']
    if len(sys.argv) > 5:
        try:
            n_speakers = int(sys.argv[5])
        except ValueError:
            print("‚ö†Ô∏è  –ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2")
            n_speakers = 2
    
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ: {input_directory}")
    print(f"üéØ –ú–æ–¥–µ–ª—å: {model_size}")
    print(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_directory}")
    print(f"üåç –Ø–∑—ã–∫: {language}")
    print(f"üé§ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è: {'–í–ö–õ–Æ–ß–ï–ù–ê' if enable_diarization else '–í–´–ö–õ–Æ–ß–ï–ù–ê'}")
    if enable_diarization:
        print(f"üë• –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö: {n_speakers}")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    use_gpu = check_gpu()
    print()
    
    # –ü–æ–∏—Å–∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
    audio_files = get_audio_files(input_directory)
    
    if not audio_files:
        print(f"‚ùå –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {input_directory}")
        print("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: wav, mp3, m4a")
        return
    
    print(f"üéµ –ù–∞–π–¥–µ–Ω–æ {len(audio_files)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤:")
    for file in audio_files:
        size_mb = os.path.getsize(file) / (1024 * 1024)
        duration = get_audio_duration(file)
        print(f"  - {os.path.basename(file)} ({size_mb:.1f} MB, {duration:.1f} —Å–µ–∫)")
    print()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, actual_device = load_whisper_model(model_size, use_gpu)
    print()
    
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –æ—á–∏—â–∞–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª
    os.makedirs(output_directory, exist_ok=True)
    
    # –û—á–∏—â–∞–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª –≤ –Ω–∞—á–∞–ª–µ
    all_txt_path = os.path.join(output_directory, "all_transcripts.txt")
    with open(all_txt_path, 'w', encoding='utf-8') as f:
        f.write("")  # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    results = []
    total_files = len(audio_files)
    
    print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {total_files} —Ñ–∞–π–ª–æ–≤ –Ω–∞ {actual_device.upper()}...\n")
    
    for i, file_path in enumerate(audio_files, 1):
        print(f"[{i}/{total_files}] ", end="")
        result = transcribe_audio(model, file_path, actual_device, language)
        
        if result:
            results.append(result)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
            save_single_result(result, output_directory, enable_diarization, model, n_speakers)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞
            if result['text']:
                preview = result['text'][:100] + "..." if len(result['text']) > 100 else result['text']
                print(f"üìù –ü—Ä–µ–≤—å—é: {preview}")
        else:
            results.append(None)
        print()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON —Ñ–∞–π–ª–∞
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –∏—Ç–æ–≥–æ–≤—ã–π JSON...")
    save_final_json(results, output_directory)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print_statistics(results)
    
    print(f"\nüéâ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_directory}/")
    print(f"üìÑ –§–∞–π–ª—ã:")
    print(f"  - all_transcripts.txt (–≤–µ—Å—å —Ç–µ–∫—Å—Ç)")
    print(f"  - transcripts_detailed.json (JSON —Å –¥–µ—Ç–∞–ª—è–º–∏)")
    print(f"  - [–∏–º—è_—Ñ–∞–π–ª–∞].txt (–æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã)")
    print(f"  - [–∏–º—è_—Ñ–∞–π–ª–∞].srt (—Å—É–±—Ç–∏—Ç—Ä—ã)")
    
    if enable_diarization:
        print(f"  - [–∏–º—è_—Ñ–∞–π–ª–∞]_diarized.* (—Ñ–∞–π–ª—ã —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π)")

if __name__ == "__main__":
    # –°–ø—Ä–∞–≤–∫–∞ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help']:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python whisper_transcribe.py [–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è] [–º–æ–¥–µ–ª—å] [–≤—ã—Ö–æ–¥–Ω–∞—è_–ø–∞–ø–∫–∞] [–¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è] [–≥–æ–≤–æ—Ä—è—â–∏–µ]")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python whisper_transcribe.py")
        print("  python whisper_transcribe.py ./audio")
        print("  python whisper_transcribe.py ./audio large ./results")
        print("  python whisper_transcribe.py ./audio large ./results true")
        print("  python whisper_transcribe.py ./audio large ./results true 3")
        print("\n–ú–æ–¥–µ–ª–∏: tiny, base, small, medium, large")
        print("–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è: true/false (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é false)")
        print("–ì–æ–≤–æ—Ä—è—â–∏–µ: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)")
        print("–ß–µ–º –±–æ–ª—å—à–µ –º–æ–¥–µ–ª—å, —Ç–µ–º —Ç–æ—á–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ")
        sys.exit(0)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
